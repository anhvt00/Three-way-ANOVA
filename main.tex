\documentclass{statistics-report}

\title{Three-way Analysis of Variance (ANOVA)}
\author{Anh T. VU}
\date{\today}

\begin{document}

\maketitle

\pagebreak

\tableofcontents

\newpage
\thispagestyle{empty}
\vfill
\begin{center}
    \textit{This page is intentionally left blank}
\end{center}
\vfill
\newpage


\section{Introduction to three-factors ANOVA}

\par \textbf{Analysis of variance (ANOVA)} is a collection of statistical models and their associated estimation procedures (such as the \textit{variation among and between groups}) used to analyze the differences among means. ANOVA was developed by the statistician Ronald Fisher. ANOVA is based on the law of total variance, where the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether two or more population means are equal, and therefore \textbf{generalizes the t-test beyond two means}.

\bigskip

\par ANOVA generalizes to the study of the effects of multiple factors. When the experiment includes observations at all combinations of levels of each factor, it is termed factorial. The use of ANOVA to study the effects of multiple factors has a complication. This essay focuses in 3 factors analysis, a simple case of the generalized multi-factors analysis problem. The terms ``three-way'', ``two-way'' or ``one-way'' in ANOVA refer to how many factors are in your test .In a 3-way ANOVA with factors $x, y$ and $z$, the ANOVA model includes terms for the \textit{main effects} $(x, y, z)$ and terms for \textit{interactions} $(xy, xz, yz, xyz)$. All terms require hypothesis tests.

\bigskip

The proliferation of interaction terms from multi-factor analysis of variance increases the risk that some hypothesis test will produce a false positive by chance. Fortunately, experience says that high order interactions are rare. The ability to detect interactions is a major advantage of multiple factor ANOVA. Testing one factor at a time hides interactions, but produces apparently inconsistent experimental results.

\bigskip

A three-way ANOVA which is also referred to as three-factor ANOVA refers to a statistical method of ascertaining three factors' effects (three independent variables) on an outcome (one dependent variable). For examples, A pharmaceutical company might execute a three-way ANOVA to ascertain a drug's effect on a medical condition. One of the factors would be the drug, another might be the gender of the subject, while the third might be the age of the subject. These three factors might each have a unique effect on the result. They might also interact with one another. The drug may positively affect male subjects, for instance, but might not work on males who are above a specific age. Three-way ANOVA permits the scientist to weigh the effects of each and if the factors interact.

\newpage
\thispagestyle{empty}
\vfill
% \begin{center}
%     \textit{This page is intentionally left blank}
% \end{center}
\vfill
\newpage

\section{History and the development of ANOVA}
While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. Around 1800, Laplace and Gauss developed the least-squares method for combining observations, which improved upon methods then used in astronomy and geodesy. It also initiated much study of the contributions to sums of squares. Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827, Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800, astronomers had isolated observational errors resulting from reaction times (the ``personal equation") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added

\bigskip

Ronald Fisher introduced the term variance and proposed its formal analysis in a 1918 article \textbf{The Correlation Between Relatives on the Supposition of Mendelian Inheritance} \cite{fisher1919xv}. His first application of the analysis of variance was published in 1921 \cite{fisher1921probable}. Analysis of variance became widely known after being included in Fisher's 1925 book Statistical Methods for Research Workers. Following its development by Sir R.A. Fisher in the 1920s, the Analysis of Variance (ANOVA) first reached a wide behavioral audience via Fisher’s 1935 book, \textbf{The Design of Experiments} \cite{fisher1937design}, a work that went through many editions and is frequently cited as the origin of behavioral science’s knowledge of ANOVA. 

\bigskip

In fact, ANOVA was first used by an educational researcher in 1934 \cite{reitz1934statistical}, based on earlier publications by Fisher. The incorporation of ANOVA was initially a gradual one, in which educational researchers and parapsychologists played the earliest roles, followed slightly later by experimental psychologists. Published applications of ANOVA remained few until after World War II, when its use accelerated rapidly. By the 1960s, it had become a routine procedure among experimental psychologists and had been incorporated as a required course in nearly all doctoral programs in psychology. 

\bigskip

The adoption of ANOVA training permitted a new generation of psychologists access to a set of tools of perceived scientific status and value, without demanding additional training in mathematics or physical science. As a result, by the 1970s, ANOVA was frequent in all experimental research, and the use of significance testing had penetrated all areas of the behavioral sciences, including those that relied upon correlational and factor-analytic techniques.

\bigskip

In the last decades of the twentieth century, ANOVA techniques displayed a greater sophistication, including repeated measures designs (\textbf{Repeated Measures Analysis of Variance}), \textbf{mixed designs, multivariate analysis of variance} and other procedures. Many of these were available long before their incorporation in psychology and other behavioral sciences. In addition, recent decades have seen a greater awareness of the formal identity between ANOVA and multiple linear regression techniques, both of which are, in effect, applications of a \textbf{generalized linear model}

\newpage
\thispagestyle{empty}
\vfill
% \begin{center}
%     \textit{This page is intentionally left blank}
% \end{center}
\vfill
\newpage

\section{Mathematical theory of three-factors ANOVA}
Base on two-factors ANOVA mathematics rationale from the books \textbf{Introduction to mathematical statistics} \cite{hogg2005introduction} and \textbf{Probability and statistical inference} \cite{hogg2010probability}, I derived the mathematics basis for ANOVA three-factors.
In this section, I am concerned with the situation where I have three factors A, B, C (independent categorical variables) with levels a, b, c, respectively and a response X (a continuous variable) depend on three factors A, B, C. This is called a \textbf{three-way} analysis of variance. Let
$X_{ijk}, i = 1,\ldots,a \quad j = 1, \ldots, b,\quad k = 1,\ldots,c$ denote the response for factor A at level
i, factor B at level j, factor C at level k. Denote the total sample size by $n = abc$. And for each combination of three levels $(i,j,k)$ of three factors (A,B,C) I obtain a sample of size $n_{ijk}$ response observations. Here I denote $(i,j,k)$ by l and $\ell = 1$ is equivalent to $(1,1,1)$, $\ell = 2$ is equivalent to $(1,1,2)$, \ldots, $\ell = abc$ is equivalent to $(a,b,c)$
    \begin{table}[htp]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \bottomrule
            Factor A & Factor B & Factor C & Response X \\
            \toprule
            \bottomrule
            1 & 1 & 1 & $x_{111}$ \\
            \hline
            1 & 1 & \ldots & $x_{11k}$\\
            \hline
            1 & 1 & c & $x_{11c}$\\
            \hline
            1 & \ldots & c & $x_{1jc}$\\            
            \hline
            1 & b & c & $x_{1bc}$\\
            \hline
            \ldots & b & c & $x_{ibc}$\\
            \hline
            a & b & c & $x_{abc}$\\
            \hline
        \end{tabular}
    \end{table}

I shall assume that the $X_{ijk}$  are independent normally distributed random variables with common variance $\sigma^{2}$. Denote the mean of $X_{ijk}$ by $\mu_{ijk}$ . The mean $\mu_{ijk}$ is often referred to as the mean of the $(i, j, k)$ cube. 
For full model, I consider the \textbf{additive model} with \textbf{main effect parameters} and \textbf{interaction parameters}
\begin{align*}
        \mu_{ijk} = \mu &+ \alpha_{i} + \beta_{j} + \gamma_{k}\\
                        &+ (\alpha\beta)_{ij} + (\alpha\gamma)_{ik} + (\beta\gamma)_{jk}\\
                        &+ (\alpha\beta\gamma)_{ijk} + \varepsilon_{ijk}
\end{align*}
in which $\alpha_{i}, \beta_{j}, \gamma_{k}$ denote main effects of factors A, B, C at levels i, j, k, respectively. With the combination of two or three effects refers to the effects of interaction of factors at respective levels. And $\varepsilon$ denotes error effect in the model.
\begin{center}
    \textbf{ANOVA effects summary of combination of three factors}
\end{center}

\begin{itemize}
    \item Main effects of 3 factors
    \begin{align*}
        &SS(T)=\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(X_{ijkl}-\bar{X})^{2} \quad &df(T) = n - 1\qquad &MS(T) = \frac{SS(T)}{df(T)}\\
        &SS(A) =\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\bar{X}_{i\ldots}-\bar{X})^{2} \quad &df(A) = a -1\qquad &MS(A) = \frac{SS(A)}{df(A)}\\
        &SS(B) =\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\bar{X}_{.j..}-\bar{X})^{2} \quad &df(B) = b -1\qquad &MS(B) = \frac{SS(B)}{df(B)}\\
        &SS(C) =\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\bar{X}_{..k.}-\bar{X})^{2} \quad &df(C) = c -1\qquad &MS(C) = \frac{SS(C)}{df(C)}\\
    \end{align*}
    \item Interactive effects of combination between 2 arbitrary factors
        \begin{align*}
            SS(AB)=\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\Bar{X}_{ij..}-\Bar{X}_{i...}-\Bar{X}_{.j..}+\Bar{X})^{2} \quad &df(AB)=(a-1)(b-1) \\
            SS(BC)=\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\Bar{X_{.jk.}}-\Bar{X_{..k.}}-\Bar{X}_{.j..}+\Bar{X})^{2} \quad &df(BC)=(b-1)(c-1) \\
            SS(CA)=\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\Bar{X}_{i.k.}-\Bar{X}_{..k.}-\Bar{X}_{i...}+\Bar{X})^{2} \quad &df(CA)=(c-1)(a-1)
        \end{align*}
        \begin{align*}
            [MS(AB)=\frac{SS(AB)}{df(AB)}\qquad
            MS(BC)=\frac{SS(BC)}{df(BC)}\qquad MS(CA)=\frac{SS(CA)}{df(CA)}
        \end{align*}
\end{itemize}


\begin{itemize}
    \item Interactive effect of all 3 factors effect
    \begin{gather*}
    SS(ABC)=\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(\bar{X}_{ijk}-\bar{X}_{ij}-\bar{X}_{jk}-\bar{X}_{ki}+\bar{X}_{i}+\bar{X}_{j}+\bar{X}_{k}-\bar{X})^{2}\\
    df(ABC) = (a-1)(b-1)(c-1)\qquad MS(ABC) = \frac{SS(ABC)}{df(ABC)}
    \end{gather*}
    \item Error effect on system
    \[
        SS(E)=\sum_{l=1}^{abc}\sum_{k=1}^{c}\sum_{j=1}^{b}\sum_{i=1}^{a}(X_{ijkl}-\bar{X}_{ijk})^{2}\qquad df(E)=n-abc\qquad MS(E)=\frac{SS(E)}{df(E)}
    \]
\end{itemize}

I will introduce the following table for the hypothesis testing with the assumptions such that
\begin{itemize}
    \item All samples are drawn from normally distributed populations
    \item All populations have a common variance
    \item All samples are drawn independently from each other
    \item Within each sample, the observations are sampled randomly and independently of each other
\end{itemize}

\begin{table}[htp]
        \begin{tabular}{|p{3.5cm}|l|p{3cm}|l|}
            \bottomrule
            Test desired & Null Hypothesis $H_{0}$ & Equivalent $H_{0}$ & Statistical test \\
            \toprule
            \bottomrule
            Effect of factor A & $\mu_{1..}=\ldots=\mu_{\alpha..}$ & $\alpha_{i}=0 \quad \forall i$ & $\frac{MS(A)}{MS(E)}\sim  F(df(A),df(E))$\\
            \hline
            Effect of factor B & $\mu_{.1.}=\ldots=\mu_{ .\beta.}$ & $\beta_{j}=0 \quad \forall j$ & $\frac{MS(B)}{MS(E)}\sim  F(df(B),df(E))$\\
            \hline
            Effect of factor C & $\mu_{.1.}=\ldots=\mu_{ ..\gamma}$ & $\gamma_{k}=0 \quad \forall k$ & $\frac{MS(C)}{MS(E)}\sim  F(df(C),df(E))$\\
            \hline
            Effect of interaction of A and B & & $\alpha\beta_{ij}=0 \quad \forall i,j$ & $\frac{MS(AB)}{MS(E)} \sim F(df(AB),df(E))$\\            
            \hline
            Effect of interaction of A and C & & $\alpha\gamma_{ik}=0 \quad \forall i,k$ & $\frac{MS(AC)}{MS(E)} \sim F(df(AC),df(E))$\\
            \hline
            Effect of interaction of B and C & & $\beta\gamma_{jk}=0 \quad \forall j,k$ & $\frac{MS(BC)}{MS(E)} \sim F(df(BC),df(E))$\\
            \hline
            Effect of interaction of A, B and C & & $\alpha\beta\gamma_{ijk}=0 \quad \forall i,j,k$ & $\frac{MS(ABC)}{MS(E)} \sim F(df(ABC),df(E))$\\
            \hline
        \end{tabular}
    \end{table}

\textbf{ANOVA 3-factor hypothesis testing framework:}
\begin{itemize}
    \item If $\frac{MS(A)}{MS(E)} > F(df(A),df(E)) \Rightarrow \textbf{reject } H_{0}(A)$  
    \item If $\frac{MS(B)}{MS(E)} > F(df(B),df(E)) \Rightarrow \textbf{reject } H_{0}(B)$
    \item If $\frac{MS(C)}{MS(E)} > F(df(C),df(E)) \Rightarrow \textbf{reject } H_{0}(C)$
    \item If $\frac{MS(AB)}{MS(E)} > F(df(AB),df(E)) \Rightarrow \textbf{reject } H_{0}(AB)$
    \item If $\frac{MS(AC)}{MS(E)} > F(df(AC),df(E)) \Rightarrow \textbf{reject } H_{0}(AC)$
    \item If $\frac{MS(BC)}{MS(E)} > F(df(BC),df(E)) \Rightarrow \textbf{reject } H_{0}(BC)$
    \item If $\frac{MS(ABC)}{MS(E)} > F(df(ABC),df(E)) \Rightarrow \textbf{reject } H_{0}(ABC)$    
\end{itemize}


\section{ANOVA 3-factor example in R}
\subsection{ANOVA assumptions}

\begin{itemize}
    \item \textbf{Independence of the observations.} Each subject should belong to only one group. There is no relationship between the observations in each group. Having repeated measures for the same participants is not allowed.
    \item \textbf{No significant outliers} in any cell of the design 
    \item \textbf{Normality.} the data for each design cell should be approximately normally distributed.
    \item \textbf{Homogeneity of variances.} The variance of the outcome variable should be equal in every cell of the design.
\end{itemize}

\subsection{Prerequisite packages}

\begin{itemize}
    \item \textbf{tidyverse} for data manipulation and visualization
    \item \textbf{ggpubr} for creating easily publication ready plots
    \item \textbf{rstatix} provides pipe-friendly R functions for easy statistical analyses
    \item \textbf{datarium}: contains required data sets for this chapter
\end{itemize}




\begin{minted}[frame=single]{r}

    library(tidyverse)
    library(ggpubr)
    library(rstatix)
    library(datarium)

\end{minted}

\subsection{Data preparation}

\par I’ll use the headache dataset [datarium package], which contains the measures of migraine headache episode pain score in 72 participants treated with three different treatments. The participants include 36 males and 36 females. Males and females were further subdivided into whether they were at low or high risk of migraine.
\smallskip
\par I want to understand how each independent variable (\textbf{type of treatments}, \textbf{risk of migraine} and \textbf{gender}) interact to predict the \textbf{pain score}.
\smallskip
\par Load the data and inspect one random row by group combinations:

\begin{minted}[frame=single]{r}
    set.seed(123)
    data("headache", package = "datarium")
    headache %>% sample_n_by(gender, risk, treatment, size = 1)
\end{minted}

\begin{figure}

    \includegraphics[scale = 0.5]{ANOVA3-TA-1.png}
    \caption{In this example, the effect of the treatment types is our focal variable, that is our primary concern. It is thought that the effect of treatments will depend on two other factors, “gender” and “risk” level of migraine, which are called moderator variables.}
    \label{fig:my_label}
\end{figure}

\subsection{Summary statistics}
\par Compute the mean and the standard deviation (SD) of pain score by groups:

\begin{minted}[frame=single]{r}
    headache %>%
    group_by(gender, risk, treatment) %>%
    get_summary_stats(pain_score, type = "mean_sd")
\end{minted}

\begin{figure}
    \includegraphics[scale = 0.5]{ANOVA3-TA-2.png}
    % \label{fig:my_label}
    \caption{Summary Statistics}
\end{figure}

\subsection{Visualization}

\par Create a box plot of pain score by treatment, color lines by risk groups and facet the plot by gender:

\begin{minted}[frame=single]{r}
    bxp <- ggboxplot(
      headache, x = "treatment", y = "pain_score", 
      color = "risk", palette = "jco", facet.by = "gender"
      )
    bxp
\end{minted}

\begin{figure}
    \includegraphics[scale = 0.5]{ANOVA3-TA-3.png}
    % \label{fig:my_label}
    \caption{Visualization}
\end{figure}

\subsection{Check assumptions}
\subsubsection{Outliers}
\par Identify outliers by groups:
\begin{minted}[frame=single]{r}
    headache %>%
      group_by(gender, risk, treatment) %>%
      identify_outliers(pain_score)
\end{minted}

\begin{figure}
    \includegraphics[scale = 0.5]{ANOVA3-TA-4.png}
    % \label{fig:my_label}
    \caption{It can be seen that, the data contain one extreme outlier (id = 57, female at high risk of migraine taking drug X)}
\end{figure}

\subsubsection{Normality assumption}
\par \textbf{Check normality assumption by analyzing the model residuals.} QQ plot and Shapiro-Wilk test of normality are used.
\begin{minted}[frame=single]{r}
    model  <- lm(pain_score ~ gender*risk*treatment, data = headache)
    # Create a QQ plot of residuals
    ggqqplot(residuals(model))
    # Compute Shapiro-Wilk test of normality
    shapiro_test(residuals(model))

\end{minted}

\begin{figure}
    \includegraphics[scale = 0.5]{ANOVA3-TA-5.png}
    % \label{fig:my_label}
    \caption{Normality terminal result}
\end{figure}

\begin{figure}
    \includegraphics[scale = 0.5]{ANOVA3-TA-6.png}
    % \label{fig:my_label}
    \caption{In the QQ plot, as all the points fall approximately along the reference line, we can assume normality. This conclusion is supported by the Shapiro-Wilk test. The p-value is not significant (p = 0.4), so we can assume normality.}
\end{figure}


\subsubsection{Homogneity of variance assumption}

\subsection{Computation}

\subsection{Post-hoc tests}
\subsubsection{Compute simple two-way interactions}

\subsubsection{Compute simple simple main effects}

\subsubsection{Compute simple simple comparisons}

\subsubsection{Report}


\bibliographystyle{abbrv}
\bibliography{ref}

\end{document}